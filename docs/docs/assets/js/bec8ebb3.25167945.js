"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[5597],{4150:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vision-language-action/lesson-4-capstone-project","title":"Lesson 4 - Capstone Project - The Autonomous Humanoid","description":"Implementing a complete autonomous humanoid system that responds to voice commands","source":"@site/docs/module-4-vision-language-action/lesson-4-capstone-project.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/lesson-4-capstone-project","permalink":"/docs/module-4-vision-language-action/lesson-4-capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/physical-ai/humanoid-robotics-book/docs/module-4-vision-language-action/lesson-4-capstone-project.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Lesson 4 - Capstone Project - The Autonomous Humanoid","sidebar_position":5,"description":"Implementing a complete autonomous humanoid system that responds to voice commands","learning_objectives":["Integrate all components from previous modules into a complete system","Implement voice command processing, planning, and execution","Handle complex scenarios with multiple requirements","Validate and test the complete autonomous humanoid system"],"duration":200},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3 - Cognitive Planning with LLMs","permalink":"/docs/module-4-vision-language-action/lesson-3-cognitive-planning-llms"}}');var t=s(4848),i=s(8453);const r={title:"Lesson 4 - Capstone Project - The Autonomous Humanoid",sidebar_position:5,description:"Implementing a complete autonomous humanoid system that responds to voice commands",learning_objectives:["Integrate all components from previous modules into a complete system","Implement voice command processing, planning, and execution","Handle complex scenarios with multiple requirements","Validate and test the complete autonomous humanoid system"],duration:200},a="Lesson 4 - Capstone Project - The Autonomous Humanoid",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Project Overview",id:"project-overview",level:2},{value:"System Architecture",id:"system-architecture",level:3},{value:"Key Components Integration",id:"key-components-integration",level:3},{value:"System Integration",id:"system-integration",level:2},{value:"Main Control Node",id:"main-control-node",level:3},{value:"Integration Architecture",id:"integration-architecture",level:3},{value:"Implementation Scenarios",id:"implementation-scenarios",level:2},{value:"Scenario 1: Basic Object Retrieval",id:"scenario-1-basic-object-retrieval",level:3},{value:"Scenario 2: Complex Multi-Step Task",id:"scenario-2-complex-multi-step-task",level:3},{value:"Scenario 3: Social Navigation",id:"scenario-3-social-navigation",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Unit Testing",id:"unit-testing",level:3},{value:"Integration Testing",id:"integration-testing",level:3},{value:"Validation Metrics",id:"validation-metrics",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Computational Efficiency",id:"computational-efficiency",level:3},{value:"Real-time Considerations",id:"real-time-considerations",level:3},{value:"Debugging and Troubleshooting",id:"debugging-and-troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Diagnostic Tools",id:"diagnostic-tools",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Self-Assessment",id:"self-assessment",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lesson-4---capstone-project---the-autonomous-humanoid",children:"Lesson 4 - Capstone Project - The Autonomous Humanoid"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this lesson, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate all components from previous modules into a complete system"}),"\n",(0,t.jsx)(n.li,{children:"Implement voice command processing, planning, and execution"}),"\n",(0,t.jsx)(n.li,{children:"Handle complex scenarios with multiple requirements"}),"\n",(0,t.jsx)(n.li,{children:"Validate and test the complete autonomous humanoid system"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project brings together all the concepts from the previous modules to create a complete autonomous humanoid system. This system will receive voice commands, process them to understand the user's intent, plan a sequence of actions, and execute those actions in the physical or simulated environment. The project demonstrates the integration of ROS 2 fundamentals, simulation, AI perception, and Vision-Language-Action systems."}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.p,{children:"The complete autonomous humanoid system includes:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[Voice Command] \u2192 [Speech Recognition] \u2192 [NLU] \u2192 [Cognitive Planning] \u2192 [Action Execution] \u2192 [Environment]\r\n     \u2193              \u2193                    \u2193         \u2193                    \u2193                   \u2193\r\n[Whisper]     [Intent Analysis]    [LLM Plan] [ROS Actions]      [Gazebo/Reality]    [Result]\n"})}),"\n",(0,t.jsx)(n.h3,{id:"key-components-integration",children:"Key Components Integration"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Processing Layer"}),": OpenAI Whisper for speech recognition"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding"}),": LLM-based intent and entity extraction"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cognitive Planning"}),": High-level action planning and reasoning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation System"}),": Nav2-based path planning and obstacle avoidance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manipulation System"}),": Object identification and grasping"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simulation Environment"}),": Gazebo for testing and validation"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"system-integration",children:"System Integration"}),"\n",(0,t.jsx)(n.h3,{id:"main-control-node",children:"Main Control Node"}),"\n",(0,t.jsx)(n.p,{children:"Creating the central coordinating node:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\nimport rospy\r\nimport actionlib\r\nfrom std_msgs.msg import String, Bool\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\r\nfrom manipulation_msgs.msg import GraspAction, GraspGoal\r\nfrom audio_common_msgs.msg import AudioData\r\nfrom capstone_msgs.msg import SystemState\r\nfrom capstone_msgs.srv import ExecuteCommand, ExecuteCommandResponse\r\n\r\nclass AutonomousHumanoidNode:\r\n    def __init__(self):\r\n        rospy.init_node(\'autonomous_humanoid\')\r\n        \r\n        # Publishers and subscribers\r\n        self.state_pub = rospy.Publisher(\'system_state\', SystemState, queue_size=1)\r\n        self.voice_cmd_sub = rospy.Subscriber(\'voice_command\', String, self.voice_command_callback)\r\n        self.whisper_sub = rospy.Subscriber(\'transcribed_text\', String, self.transcription_callback)\r\n        \r\n        # Action clients\r\n        self.move_base_client = actionlib.SimpleActionClient(\'move_base\', MoveBaseAction)\r\n        self.grasp_client = actionlib.SimpleActionClient(\'grasp_server\', GraspAction)\r\n        \r\n        # Services\r\n        self.execute_srv = rospy.Service(\'execute_command\', ExecuteCommand, self.execute_command)\r\n        \r\n        # Internal state\r\n        self.current_command = None\r\n        self.system_state = SystemState()\r\n        self.system_state.status = "IDLE"\r\n        \r\n        # Wait for action servers\r\n        rospy.loginfo("Waiting for action servers...")\r\n        self.move_base_client.wait_for_server()\r\n        self.grasp_client.wait_for_server()\r\n        \r\n        rospy.loginfo("Autonomous Humanoid system initialized")\r\n    \r\n    def voice_command_callback(self, msg):\r\n        """Handle voice command from user"""\r\n        rospy.loginfo(f"Received voice command: {msg.data}")\r\n        self.system_state.status = "PROCESSING"\r\n        self.state_pub.publish(self.system_state)\r\n        \r\n        # Send command to planning system\r\n        self.process_command(msg.data)\r\n    \r\n    def transcription_callback(self, msg):\r\n        """Handle transcribed text from Whisper"""\r\n        rospy.loginfo(f"Transcribed: {msg.data}")\r\n        # Process the transcribed command\r\n        self.process_command(msg.data)\r\n    \r\n    def process_command(self, command_text):\r\n        """Process natural language command"""\r\n        try:\r\n            # Use LLM to interpret command and generate plan\r\n            plan = self.generate_plan_from_command(command_text)\r\n            \r\n            # Execute the plan\r\n            success = self.execute_plan(plan)\r\n            \r\n            if success:\r\n                self.system_state.status = "IDLE"\r\n                rospy.loginfo("Command executed successfully")\r\n            else:\r\n                self.system_state.status = "ERROR"\r\n                rospy.logerr("Command execution failed")\r\n                \r\n        except Exception as e:\r\n            rospy.logerr(f"Error processing command: {e}")\r\n            self.system_state.status = "ERROR"\r\n    \r\n    def generate_plan_from_command(self, command):\r\n        """Generate action plan from natural language command"""\r\n        # This would interface with your LLM-based planning system\r\n        # For this example, we\'ll handle some basic commands\r\n        command_lower = command.lower()\r\n        \r\n        if "go to" in command_lower:\r\n            location = self.extract_location(command)\r\n            return [("navigate", location)]\r\n        elif "pick up" in command_lower or "grasp" in command_lower:\r\n            object_name = self.extract_object(command)\r\n            return [("grasp", object_name)]\r\n        elif "bring me" in command_lower:\r\n            parts = command_lower.split("bring me")\r\n            object_name = self.extract_object(parts[1])\r\n            return [("grasp", object_name), ("navigate", "user")]\r\n        else:\r\n            # Use LLM for complex command planning\r\n            return self.request_complex_plan(command)\r\n    \r\n    def execute_plan(self, plan):\r\n        """Execute the action plan"""\r\n        try:\r\n            for action_type, params in plan:\r\n                if action_type == "navigate":\r\n                    success = self.navigate_to_location(params)\r\n                elif action_type == "grasp":\r\n                    success = self.grasp_object(params)\r\n                else:\r\n                    rospy.logwarn(f"Unknown action type: {action_type}")\r\n                    success = False\r\n                \r\n                if not success:\r\n                    rospy.logerr(f"Action failed: {action_type} {params}")\r\n                    return False\r\n            \r\n            return True\r\n        except Exception as e:\r\n            rospy.logerr(f"Error executing plan: {e}")\r\n            return False\r\n    \r\n    def navigate_to_location(self, location):\r\n        """Navigate to a specific location"""\r\n        # Convert location name to coordinates (would use map lookup)\r\n        pose = self.location_to_pose(location)\r\n        \r\n        if not pose:\r\n            rospy.logerr(f"Unknown location: {location}")\r\n            return False\r\n        \r\n        goal = MoveBaseGoal()\r\n        goal.target_pose = pose\r\n        \r\n        rospy.loginfo(f"Navigating to {location}")\r\n        self.move_base_client.send_goal(goal)\r\n        \r\n        # Wait for result with timeout\r\n        finished_within_time = self.move_base_client.wait_for_result(rospy.Duration(60.0))\r\n        \r\n        if not finished_within_time:\r\n            rospy.logerr("Navigation took too long")\r\n            self.move_base_client.cancel_goal()\r\n            return False\r\n        \r\n        state = self.move_base_client.get_state()\r\n        success = (state == actionlib.GoalStatus.SUCCEEDED)\r\n        \r\n        if success:\r\n            rospy.loginfo(f"Successfully reached {location}")\r\n        else:\r\n            rospy.logerr(f"Failed to reach {location}")\r\n        \r\n        return success\r\n    \r\n    def grasp_object(self, object_name):\r\n        """Grasp a specific object"""\r\n        goal = GraspGoal()\r\n        goal.object_name = object_name\r\n        \r\n        rospy.loginfo(f"Attempting to grasp {object_name}")\r\n        self.grasp_client.send_goal(goal)\r\n        \r\n        # Wait for result with timeout\r\n        finished_within_time = self.grasp_client.wait_for_result(rospy.Duration(30.0))\r\n        \r\n        if not finished_within_time:\r\n            rospy.logerr(f"Grasping {object_name} took too long")\r\n            self.grasp_client.cancel_goal()\r\n            return False\r\n        \r\n        state = self.grasp_client.get_state()\r\n        success = (state == actionlib.GoalStatus.SUCCEEDED)\r\n        \r\n        if success:\r\n            rospy.loginfo(f"Successfully grasped {object_name}")\r\n        else:\r\n            rospy.logerr(f"Failed to grasp {object_name}")\r\n        \r\n        return success\r\n    \r\n    def execute_command(self, req):\r\n        """Service to execute command directly"""\r\n        success = self.process_command_with_context(req.command, req.context)\r\n        return ExecuteCommandResponse(success=success)\r\n    \r\n    def location_to_pose(self, location_name):\r\n        """Convert location name to PoseStamped"""\r\n        # This would typically look up coordinates from a map\r\n        # For this example, we\'ll use hardcoded locations\r\n        locations = {\r\n            "kitchen": PoseStamped(),  # Filled with appropriate coordinates\r\n            "living room": PoseStamped(),\r\n            "bedroom": PoseStamped(),\r\n            "user": self.get_user_pose()  # Would get current user position\r\n        }\r\n        \r\n        return locations.get(location_name.lower())\r\n    \r\n    def get_user_pose(self):\r\n        """Get current user position (placeholder)"""\r\n        # In a real system, this might use person detection or tracking\r\n        pose = PoseStamped()\r\n        # Set appropriate coordinates\r\n        return pose\r\n    \r\n    def extract_location(self, command):\r\n        """Extract location from command (simplified)"""\r\n        # This would use more sophisticated NLU in practice\r\n        if "kitchen" in command.lower():\r\n            return "kitchen"\r\n        elif "living room" in command.lower():\r\n            return "living room"\r\n        elif "bedroom" in command.lower():\r\n            return "bedroom"\r\n        else:\r\n            return "unknown"\r\n    \r\n    def extract_object(self, command):\r\n        """Extract object from command (simplified)"""\r\n        # This would use more sophisticated NLU in practice\r\n        # For now, just return a simple object\r\n        if "cup" in command.lower():\r\n            return "cup"\r\n        elif "book" in command.lower():\r\n            return "book"\r\n        else:\r\n            return "object"\r\n    \r\n    def request_complex_plan(self, command):\r\n        """Request complex plan from LLM system"""\r\n        # Interface with your LLM planning system\r\n        # This would send the command to your LLM planning system\r\n        # and receive back a sequence of actions\r\n        rospy.loginfo(f"Requesting plan for: {command}")\r\n        # Placeholder - would call LLM planning system\r\n        return [("navigate", "location"), ("grasp", "object")]\r\n    \r\n    def run(self):\r\n        """Run the main loop"""\r\n        rate = rospy.Rate(1)  # 1 Hz\r\n        while not rospy.is_shutdown():\r\n            # Update system state\r\n            self.state_pub.publish(self.system_state)\r\n            rate.sleep()\r\n\r\nif __name__ == \'__main__\':\r\n    node = AutonomousHumanoidNode()\r\n    node.run()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"integration-architecture",children:"Integration Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Creating launch files to bring up the complete system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'\x3c!-- capstone_launch.xml --\x3e\r\n<launch>\r\n  \x3c!-- Simulation environment --\x3e\r\n  <include file="$(find gazebo_ros)/launch/empty_world.launch">\r\n    <arg name="world_name" value="$(find capstone_project)/worlds/indoor_world.world" />\r\n  </include>\r\n  \r\n  \x3c!-- Robot spawn --\x3e\r\n  <node name="spawn_urdf" pkg="gazebo_ros" type="spawn_model" args="-file $(find capstone_project)/robots/humanoid.urdf -urdf -model humanoid_robot" />\r\n  \r\n  \x3c!-- Navigation stack --\x3e\r\n  <include file="$(find nav2_bringup)/launch/navigation_launch.py">\r\n    <arg name="use_sim_time" value="True"/>\r\n  </include>\r\n  \r\n  \x3c!-- Speech recognition --\x3e\r\n  <node pkg="audio_capture" exec="audio_capture_node" name="audio_capture" output="screen">\r\n    <param name="device" value="default" />\r\n    <param name="sample_rate" value="16000" />\r\n  </node>\r\n  \r\n  <node pkg="whisper_ros" exec="whisper_node" name="whisper_node" output="screen">\r\n    <param name="model_size" value="base" />\r\n    <param name="language" value="en" />\r\n  </node>\r\n  \r\n  \x3c!-- Cognitive planning --\x3e\r\n  <node pkg="llm_planner" exec="llm_planner_node" name="llm_planner" output="screen">\r\n    <param name="model_name" value="gpt-3.5-turbo" />\r\n  </node>\r\n  \r\n  \x3c!-- Main control node --\x3e\r\n  <node pkg="capstone_project" exec="autonomous_humanoid_node" name="autonomous_humanoid" output="screen">\r\n  </node>\r\n  \r\n  \x3c!-- Visualization --\x3e\r\n  <node name="rviz" pkg="rviz2" exec="rviz2" args="-d $(find capstone_project)/rviz/capstone_config.rviz" />\r\n</launch>\n'})}),"\n",(0,t.jsx)(n.h2,{id:"implementation-scenarios",children:"Implementation Scenarios"}),"\n",(0,t.jsx)(n.h3,{id:"scenario-1-basic-object-retrieval",children:"Scenario 1: Basic Object Retrieval"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Command"}),': "Go to the kitchen and bring me a cup"']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Execution Flow"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Voice command captured and transcribed"}),"\n",(0,t.jsx)(n.li,{children:'Intent identified as "fetch_object" with location "kitchen" and object "cup"'}),"\n",(0,t.jsx)(n.li,{children:"Navigation system plans path to kitchen"}),"\n",(0,t.jsx)(n.li,{children:"Robot navigates to kitchen and identifies available cups"}),"\n",(0,t.jsx)(n.li,{children:"Manipulation system plans and executes grasp of cup"}),"\n",(0,t.jsx)(n.li,{children:"Robot navigates back to user"}),"\n",(0,t.jsx)(n.li,{children:"Manipulation system places cup near user"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"scenario-2-complex-multi-step-task",children:"Scenario 2: Complex Multi-Step Task"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Command"}),': "Clean the table in the living room"']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Execution Flow"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"LLM-based planning decomposes task into sub-tasks"}),"\n",(0,t.jsx)(n.li,{children:"Navigate to living room table"}),"\n",(0,t.jsx)(n.li,{children:"Identify objects on table"}),"\n",(0,t.jsxs)(n.li,{children:["For each movable object:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Identify appropriate storage location"}),"\n",(0,t.jsx)(n.li,{children:"Grasp object"}),"\n",(0,t.jsx)(n.li,{children:"Navigate to storage location"}),"\n",(0,t.jsx)(n.li,{children:"Place object"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Verify table is clear"}),"\n",(0,t.jsx)(n.li,{children:"Report completion"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"scenario-3-social-navigation",children:"Scenario 3: Social Navigation"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Command"}),': "Go to the front door, but don\'t bump into anyone"']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Execution Flow"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Identify front door location"}),"\n",(0,t.jsx)(n.li,{children:"Plan path that avoids obstacles and people"}),"\n",(0,t.jsx)(n.li,{children:"Use social navigation behaviors to respect personal space"}),"\n",(0,t.jsx)(n.li,{children:"Navigate while monitoring for dynamic obstacles"}),"\n",(0,t.jsx)(n.li,{children:"Adjust path planning in real-time as people move"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,t.jsx)(n.h3,{id:"unit-testing",children:"Unit Testing"}),"\n",(0,t.jsx)(n.p,{children:"Test individual components:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import unittest\r\nimport rospy\r\nfrom capstone_msgs.srv import ExecuteCommand\r\n\r\nclass TestAutonomousHumanoid(unittest.TestCase):\r\n    def setUp(self):\r\n        rospy.init_node(\'test_autonomous_humanoid\', anonymous=True)\r\n        rospy.wait_for_service(\'execute_command\')\r\n        self.execute_command = rospy.ServiceProxy(\'execute_command\', ExecuteCommand)\r\n    \r\n    def test_simple_navigation(self):\r\n        """Test simple navigation command"""\r\n        response = self.execute_command("Go to the kitchen", "{}")\r\n        self.assertTrue(response.success)\r\n    \r\n    def test_object_fetch(self):\r\n        """Test object fetching command"""\r\n        response = self.execute_command("Bring me a cup", "{}")\r\n        self.assertTrue(response.success)\r\n    \r\n    def test_complex_task(self):\r\n        """Test complex multi-step task"""\r\n        response = self.execute_command("Clean the table", "{}")\r\n        self.assertTrue(response.success)\r\n\r\nif __name__ == \'__main__\':\r\n    import rosunit\r\n    rosunit.unitrun(\'capstone_project\', \'test_autonomous_humanoid\', TestAutonomousHumanoid)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,t.jsx)(n.p,{children:"Test complete system functionality:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# Test scenarios for integration testing\r\ntest_scenarios:\r\n  - name: "basic_navigation"\r\n    command: "Go to the kitchen"\r\n    expected: ["navigation_success"]\r\n    timeout: 60\r\n    \r\n  - name: "object_retrieval"\r\n    command: "Bring me a cup"\r\n    expected: ["navigation_to_kitchen", "object_grasped", "navigation_to_user", "object_delivered"]\r\n    timeout: 120\r\n    \r\n  - name: "multi_step_task"\r\n    command: "Go to living room, pick up the book, and bring it to me"\r\n    expected: ["navigation_to_living_room", "object_grasped", "navigation_to_user", "object_delivered"]\r\n    timeout: 180\r\n    \r\n  - name: "social_navigation"\r\n    command: "Navigate to front door while avoiding people"\r\n    expected: ["navigation_with_avoidance", "safe_path_completion"]\r\n    timeout: 90\n'})}),"\n",(0,t.jsx)(n.h3,{id:"validation-metrics",children:"Validation Metrics"}),"\n",(0,t.jsx)(n.p,{children:"Track system performance:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task Success Rate"}),": Percentage of tasks completed successfully"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Time Efficiency"}),": Time taken to complete tasks vs. optimal"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety Metrics"}),": Number of safety violations or near-misses"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"User Satisfaction"}),": User feedback on system performance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"System Reliability"}),": Time between system failures"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"computational-efficiency",children:"Computational Efficiency"}),"\n",(0,t.jsx)(n.p,{children:"Optimize system performance:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parallel Processing"}),": Execute independent tasks simultaneously"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Caching"}),": Cache frequently used plans and information"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Model Optimization"}),": Use quantized or distilled models where possible"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Resource Management"}),": Monitor and balance computational resources"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"real-time-considerations",children:"Real-time Considerations"}),"\n",(0,t.jsx)(n.p,{children:"Ensure real-time operation:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency Management"}),": Minimize delays between command and action"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Priority Scheduling"}),": Ensure safety-critical operations take precedence"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Recovery"}),": Gracefully handle failures without system crashes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"State Monitoring"}),": Continuously monitor system state for anomalies"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"debugging-and-troubleshooting",children:"Debugging and Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech Recognition Errors"}),": Implement confidence thresholds and re-asking mechanisms"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation Failures"}),": Use multiple planning strategies and recovery behaviors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Detection Issues"}),": Implement robust object identification with multiple sensors"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Communication Problems"}),": Ensure ROS communication reliability"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"diagnostic-tools",children:"Diagnostic Tools"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Diagnostic node to monitor system health\r\nimport rospy\r\nfrom diagnostic_msgs.msg import DiagnosticArray, DiagnosticStatus, KeyValue\r\n\r\nclass CapstoneDiagnostics:\r\n    def __init__(self):\r\n        rospy.init_node(\'capstone_diagnostics\')\r\n        self.diag_pub = rospy.Publisher(\'/diagnostics\', DiagnosticArray, queue_size=1)\r\n        self.timer = rospy.Timer(rospy.Duration(1.0), self.publish_diagnostics)\r\n        \r\n    def publish_diagnostics(self, event):\r\n        msg = DiagnosticArray()\r\n        msg.header.stamp = rospy.Time.now()\r\n        \r\n        # System health\r\n        status = DiagnosticStatus()\r\n        status.name = "Capstone System Health"\r\n        status.level = DiagnosticStatus.OK\r\n        status.message = "All systems operational"\r\n        \r\n        # Add key-value pairs for system stats\r\n        status.values.append(KeyValue("navigation_status", self.get_navigation_status()))\r\n        status.values.append(KeyValue("manipulation_status", self.get_manipulation_status()))\r\n        status.values.append(KeyValue("speech_recognition_status", self.get_speech_status()))\r\n        status.values.append(KeyValue("battery_level", self.get_battery_level()))\r\n        \r\n        msg.status.append(status)\r\n        self.diag_pub.publish(msg)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"System Assembly"}),": Assemble the complete system using previously created components:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Launch all required nodes (simulation, speech, planning, control)"}),"\n",(0,t.jsx)(n.li,{children:"Verify that all components can communicate"}),"\n",(0,t.jsx)(n.li,{children:"Test individual subsystems before integration"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Basic Command Test"}),": Test the system with simple commands:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"Go to the kitchen"'}),"\n",(0,t.jsx)(n.li,{children:'"Move forward"'}),"\n",(0,t.jsx)(n.li,{children:'"Turn left"'}),"\n",(0,t.jsx)(n.li,{children:"Verify each command executes correctly"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Object Retrieval"}),": Test the complete object retrieval scenario:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"Go to the kitchen and bring me a cup"'}),"\n",(0,t.jsx)(n.li,{children:"Monitor the entire process: navigation, object detection, grasping, and delivery"}),"\n",(0,t.jsx)(n.li,{children:"Verify safety and success criteria"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Complex Task Execution"}),": Execute the complex multi-step task:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"Clean the table"'}),"\n",(0,t.jsx)(n.li,{children:"Monitor decomposition, planning, and execution of multiple subtasks"}),"\n",(0,t.jsx)(n.li,{children:"Verify the completion criteria are met"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Performance Evaluation"}),": Evaluate system performance:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Measure task success rates"}),"\n",(0,t.jsx)(n.li,{children:"Track time efficiency"}),"\n",(0,t.jsx)(n.li,{children:"Assess user satisfaction"}),"\n",(0,t.jsx)(n.li,{children:"Document any system limitations or issues"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Error Handling"}),": How would you implement robust error handling for when the robot fails to grasp an object or navigate to a location?"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"System Scaling"}),": How would you modify the system to handle multiple simultaneous commands or operate with multiple robots?"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Learning Integration"}),": How could you add learning capabilities to improve system performance based on experience?"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Real-World Deployment"}),": What additional considerations would be needed to deploy this system in a real-world environment?"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project demonstrates the integration of all components from the Physical AI & Humanoid Robotics course. It showcases how ROS 2 fundamentals, simulation, AI perception systems, and Vision-Language-Action frameworks combine to create an intelligent, autonomous humanoid robot capable of understanding and executing natural language commands."}),"\n",(0,t.jsx)(n.h2,{id:"self-assessment",children:"Self-Assessment"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"How do the different modules of this course integrate in the capstone project?"}),"\n",(0,t.jsx)(n.li,{children:"What are the main challenges in creating an autonomous humanoid system?"}),"\n",(0,t.jsx)(n.li,{children:"How would you validate the safety and reliability of such a system?"}),"\n",(0,t.jsx)(n.li,{children:"What are the key performance metrics for evaluating the complete system?"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>a});var o=s(6540);const t={},i=o.createContext(t);function r(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);