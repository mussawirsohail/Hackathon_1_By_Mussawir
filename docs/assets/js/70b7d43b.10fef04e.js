"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[16],{3223:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vision-language-action/intro","title":"Introduction to Module 4 - Vision-Language-Action (VLA)","description":"The convergence of LLMs and Robotics for humanoid systems","source":"@site/docs/module-4-vision-language-action/intro.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/intro","permalink":"/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to Module 4 - Vision-Language-Action (VLA)","sidebar_position":1,"description":"The convergence of LLMs and Robotics for humanoid systems"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 4 - Nav2 for Bipedal Humanoid Navigation","permalink":"/Hackathon_1_By_Mussawir/docs/module-3-the-ai-robot-brain/lesson-4-path-planning-for-bipedal-movement"},"next":{"title":"Lesson 1 - Introduction to Vision-Language-Action Systems","permalink":"/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-1-introduction-to-vla"}}');var t=i(4848),s=i(8453);const a={title:"Introduction to Module 4 - Vision-Language-Action (VLA)",sidebar_position:1,description:"The convergence of LLMs and Robotics for humanoid systems"},r="Module 4: Vision-Language-Action (VLA)",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Duration",id:"duration",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Welcome to Module 4 of the Physical AI & Humanoid Robotics book. This module explores the emerging field of Vision-Language-Action (VLA) systems, where large language models (LLMs) are integrated with computer vision and robotic control to create systems that can understand natural language commands and execute complex robotic tasks. This convergence represents the next frontier in humanoid robotics."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand the architecture of Vision-Language-Action systems"}),"\n",(0,t.jsx)(n.li,{children:"Integrate speech recognition with robotic command interpretation"}),"\n",(0,t.jsx)(n.li,{children:"Implement cognitive planning systems that translate natural language to robotic actions"}),"\n",(0,t.jsx)(n.li,{children:"Execute a comprehensive capstone project integrating all system components"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,t.jsx)(n.p,{children:"This module contains four lessons that build upon each other:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Lesson 1"}),": Introduction to VLA - Understanding the convergence of vision, language, and action"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Lesson 2"}),": Voice-to-Action - Using OpenAI Whisper for voice command processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Lesson 3"}),": Cognitive Planning - Using LLMs to translate natural language into ROS 2 actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Lesson 4"}),": Capstone Project - The Autonomous Humanoid completing an integrated task"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before starting this module, you should have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed Modules 1-3 (ROS 2, simulation, and AI systems)"}),"\n",(0,t.jsx)(n.li,{children:"Basic understanding of machine learning and neural networks"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with natural language processing concepts"}),"\n",(0,t.jsx)(n.li,{children:"Programming experience with Python and AI frameworks"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"duration",children:"Duration"}),"\n",(0,t.jsx)(n.p,{children:"Estimated time to complete: 15-20 hours"}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"Begin with Lesson 1 to understand the fundamentals of Vision-Language-Action systems and their applications in humanoid robotics."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var o=i(6540);const t={},s=o.createContext(t);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);