"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[1137],{8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>r});var s=i(6540);const t={},a=s.createContext(t);function o(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),s.createElement(a.Provider,{value:e},n.children)}},8765:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4-vision-language-action/lesson-3-cognitive-planning-llms","title":"Lesson 3 - Cognitive Planning with LLMs","description":"Using Large Language Models to translate natural language into robotic actions","source":"@site/docs/module-4-vision-language-action/lesson-3-cognitive-planning-llms.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/lesson-3-cognitive-planning-llms","permalink":"/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-3-cognitive-planning-llms","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-3-cognitive-planning-llms.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Lesson 3 - Cognitive Planning with LLMs","sidebar_position":4,"description":"Using Large Language Models to translate natural language into robotic actions","learning_objectives":["Implement LLM-based cognitive planning for robotic tasks","Design prompt engineering techniques for robotic applications","Integrate LLMs with ROS 2 action planning systems","Validate and verify LLM-generated action sequences"],"duration":180},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2 - Voice-to-Action with OpenAI Whisper","permalink":"/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-2-voice-to-action-whisper"},"next":{"title":"Lesson 4 - Capstone Project - The Autonomous Humanoid","permalink":"/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-4-capstone-project"}}');var t=i(4848),a=i(8453);const o={title:"Lesson 3 - Cognitive Planning with LLMs",sidebar_position:4,description:"Using Large Language Models to translate natural language into robotic actions",learning_objectives:["Implement LLM-based cognitive planning for robotic tasks","Design prompt engineering techniques for robotic applications","Integrate LLMs with ROS 2 action planning systems","Validate and verify LLM-generated action sequences"],duration:180},r="Lesson 3 - Cognitive Planning with LLMs",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"LLM Architecture for Robotics",id:"llm-architecture-for-robotics",level:2},{value:"Language Models in Robotics",id:"language-models-in-robotics",level:3},{value:"Robotic-Specific Models",id:"robotic-specific-models",level:3},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:2},{value:"System Prompt Design",id:"system-prompt-design",level:3},{value:"Few-Shot Examples",id:"few-shot-examples",level:3},{value:"Context-Aware Prompting",id:"context-aware-prompting",level:3},{value:"Integration with ROS 2 Planning Systems",id:"integration-with-ros-2-planning-systems",level:2},{value:"Action Space Mapping",id:"action-space-mapping",level:3},{value:"Planning Interface",id:"planning-interface",level:3},{value:"Safety and Validation",id:"safety-and-validation",level:2},{value:"Safety Constraints",id:"safety-constraints",level:3},{value:"Validation Techniques",id:"validation-techniques",level:3},{value:"Example Safety Validation",id:"example-safety-validation",level:3},{value:"Cognitive Planning Strategies",id:"cognitive-planning-strategies",level:2},{value:"Hierarchical Planning",id:"hierarchical-planning",level:3},{value:"Contextual Reasoning",id:"contextual-reasoning",level:3},{value:"Learning from Interaction",id:"learning-from-interaction",level:3},{value:"Integration with Perception Systems",id:"integration-with-perception-systems",level:2},{value:"Real-Time Perception Integration",id:"real-time-perception-integration",level:3},{value:"Feedback Loops",id:"feedback-loops",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching Strategies",id:"caching-strategies",level:3},{value:"Parallel Processing",id:"parallel-processing",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Plan Quality Metrics",id:"plan-quality-metrics",level:3},{value:"LLM Performance Metrics",id:"llm-performance-metrics",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Self-Assessment",id:"self-assessment",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"lesson-3---cognitive-planning-with-llms",children:"Lesson 3 - Cognitive Planning with LLMs"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"After completing this lesson, you will be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Implement LLM-based cognitive planning for robotic tasks"}),"\n",(0,t.jsx)(e.li,{children:"Design prompt engineering techniques for robotic applications"}),"\n",(0,t.jsx)(e.li,{children:"Integrate LLMs with ROS 2 action planning systems"}),"\n",(0,t.jsx)(e.li,{children:"Validate and verify LLM-generated action sequences"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(e.p,{children:"Large Language Models (LLMs) have emerged as powerful tools for cognitive planning in robotics, enabling robots to interpret natural language commands and decompose them into executable action sequences. For humanoid robots, LLMs serve as the cognitive bridge between high-level human instructions and low-level robotic actions, providing the reasoning capabilities needed to navigate complex real-world tasks. This lesson covers implementing LLM-driven planning systems with a focus on safety, accuracy, and real-world applicability."}),"\n",(0,t.jsx)(e.h2,{id:"llm-architecture-for-robotics",children:"LLM Architecture for Robotics"}),"\n",(0,t.jsx)(e.h3,{id:"language-models-in-robotics",children:"Language Models in Robotics"}),"\n",(0,t.jsx)(e.p,{children:"Modern LLMs for robotics typically include:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Transformer Architecture"}),": Self-attention mechanisms for understanding context"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Multimodal Capabilities"}),": Integration of vision and language models"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Instruction Following"}),": Training to follow complex multi-step instructions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Chain-of-Thought Reasoning"}),": Step-by-step reasoning for complex tasks"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"robotic-specific-models",children:"Robotic-Specific Models"}),"\n",(0,t.jsx)(e.p,{children:"Specialized models for robotics include:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"RT-2 (Robotics Transformer 2)"}),": Maps language to robot actions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Instruct2Act"}),": Translates natural language to robotic actions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"SayCan"}),": Combines language understanding with action selection"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"VoxPoser"}),": Vision-language reasoning for manipulation"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,t.jsx)(e.h3,{id:"system-prompt-design",children:"System Prompt Design"}),"\n",(0,t.jsx)(e.p,{children:"Designing effective system prompts for robotic applications:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'SYSTEM_PROMPT = """\nYou are a robotic planning assistant. You will receive natural language commands\nand must generate step-by-step plans for a humanoid robot to execute.\n\nRobot capabilities:\n- Navigate to locations in the environment\n- Detect and identify objects\n- Manipulate objects (pick up, place, hand over)\n- Open/close doors and drawers\n- Turn devices on/off\n- Communicate with humans through speech\n\nOutput format:\n<thinking>\nBreak down the command and consider possible approaches.\nConsider safety, feasibility, and environmental constraints.\n</thinking>\n\n<action_sequence>\n1. [Action 1]\n2. [Action 2]\n3. [Action 3]\n...\n</action_sequence>\n\nAvailable actions:\n- NAVIGATE_TO(location)\n- DETECT_OBJECT(object_type)\n- GRASP_OBJECT(object_name)\n- PLACE_OBJECT(location)\n- OPEN_CONTAINER(container_name)\n- CLOSE_CONTAINER(container_name)\n- TURN_ON(device_name)\n- TURN_OFF(device_name)\n- SPEAK(text)\n- WAIT_FOR(condition)\n"""\n'})}),"\n",(0,t.jsx)(e.h3,{id:"few-shot-examples",children:"Few-Shot Examples"}),"\n",(0,t.jsx)(e.p,{children:"Providing examples to guide LLM behavior:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'FEW_SHOT_EXAMPLES = [\n    {\n        "input": "Go to the kitchen and bring me a cup of coffee.",\n        "output": """\n<thinking>\nThe user wants a cup of coffee from the kitchen.\n1. I need to navigate to the kitchen\n2. Find a cup\n3. Find coffee (or make coffee if possible)\n4. Pick up the cup with coffee\n5. Navigate back to the user\nThe robot likely cannot make coffee, so I\'ll assume there\'s already coffee available.\n</thinking>\n\n<action_sequence>\n1. NAVIGATE_TO(kitchen)\n2. DETECT_OBJECT(cup)\n3. DETECT_OBJECT(coffee)\n4. GRASP_OBJECT(cup)\n5. PLACE_OBJECT(user_location)\n</action_sequence>\n        """\n    },\n    {\n        "input": "Clean the table.",\n        "output": """\n<thinking>\nThe user wants the table cleaned. This likely means removing objects from the table.\n1. I need to identify objects on the table\n2. Determine where to place these objects\n3. Remove objects systematically\n4. Check if cleaning is complete\n</thinking>\n\n<action_sequence>\n1. NAVIGATE_TO(table)\n2. DETECT_OBJECTS(table_surface)\n3. FOR each object in detected_objects:\n   - DETERMINE appropriate_storage_location(object_type)\n   - GRASP_OBJECT(object)\n   - PLACE_OBJECT(storage_location)\n4. SPEAK(text="The table has been cleaned")\n</action_sequence>\n        """\n    }\n]\n'})}),"\n",(0,t.jsx)(e.h3,{id:"context-aware-prompting",children:"Context-Aware Prompting"}),"\n",(0,t.jsx)(e.p,{children:"Incorporating environmental context into prompts:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Current State"}),": Robot's location, battery level, recent actions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"World Knowledge"}),": Object locations, room layout, affordances"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"User Context"}),": Preferences, previous interactions, current activity"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety Constraints"}),": Forbidden actions, hazardous areas"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"integration-with-ros-2-planning-systems",children:"Integration with ROS 2 Planning Systems"}),"\n",(0,t.jsx)(e.h3,{id:"action-space-mapping",children:"Action Space Mapping"}),"\n",(0,t.jsx)(e.p,{children:"Mapping LLM outputs to ROS 2 action sequences:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'ACTION_MAP = {\n    "NAVIGATE_TO": {\n        "ros_action": "nav2_msgs/action/NavigateToPose",\n        "params": ["x", "y", "theta", "frame_id"],\n        "description": "Navigate to a specific pose in the environment"\n    },\n    "GRASP_OBJECT": {\n        "ros_action": "manipulation_msgs/action/Grasp",\n        "params": ["object_name", "grasp_type"],\n        "description": "Grasp an object with specified grasp type"\n    },\n    "SPEAK": {\n        "ros_action": "sound_play_msgs/action/Speak",\n        "params": ["text"],\n        "description": "Speak text using text-to-speech"\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"planning-interface",children:"Planning Interface"}),"\n",(0,t.jsx)(e.p,{children:"Creating a planning interface for LLM integration:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rospy\nfrom llm_planner_msgs.srv import PlanFromNaturalLanguage\nfrom llm_planner_msgs.msg import LLMAction\nfrom std_msgs.msg import String\n\nclass LLMPlannerNode:\n    def __init__(self):\n        rospy.init_node(\'llm_planner_node\')\n        \n        # Service for receiving natural language commands\n        self.plan_service = rospy.Service(\n            \'plan_from_natural_language\',\n            PlanFromNaturalLanguage,\n            self.plan_callback\n        )\n        \n        # Publisher for action sequences\n        self.action_pub = rospy.Publisher(\n            \'planned_actions\',\n            LLMAction,\n            queue_size=10\n        )\n        \n        # Publisher for status updates\n        self.status_pub = rospy.Publisher(\n            \'planner_status\',\n            String,\n            queue_size=10\n        )\n        \n        # Initialize LLM client\n        self.llm_client = self.initialize_llm_client()\n        \n        rospy.loginfo("LLM Planner node initialized")\n\n    def plan_callback(self, req):\n        """Process natural language command and generate action plan"""\n        try:\n            # Format the command for LLM\n            formatted_command = self.format_command(\n                req.natural_language_command,\n                req.context_info\n            )\n            \n            # Generate plan using LLM\n            plan = self.generate_plan_with_llm(formatted_command)\n            \n            # Validate the plan for safety and feasibility\n            validated_plan = self.validate_plan(plan)\n            \n            # Publish the action sequence\n            for action in validated_plan:\n                self.action_pub.publish(action)\n            \n            # Return success response\n            return PlanFromNaturalLanguageResponse(True, validated_plan)\n            \n        except Exception as e:\n            rospy.logerr(f"Error generating plan: {e}")\n            return PlanFromNaturalLanguageResponse(False, [])\n    \n    def format_command(self, command, context):\n        """Format command with context for LLM"""\n        return f"""\nCurrent context: {context}\nUser command: {command}\n\nGenerate a step-by-step plan for the robot to execute this command.\n        """\n    \n    def generate_plan_with_llm(self, formatted_command):\n        """Generate plan using LLM"""\n        response = self.llm_client.generate(formatted_command)\n        return self.parse_llm_response(response)\n    \n    def validate_plan(self, plan):\n        """Validate plan for safety and feasibility"""\n        validated_plan = []\n        for action in plan:\n            if self.is_action_safe(action) and self.is_action_feasible(action):\n                validated_plan.append(action)\n            else:\n                rospy.logwarn(f"Invalid action removed: {action}")\n        \n        return validated_plan\n'})}),"\n",(0,t.jsx)(e.h2,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,t.jsx)(e.h3,{id:"safety-constraints",children:"Safety Constraints"}),"\n",(0,t.jsx)(e.p,{children:"Implementing safety checks for LLM-generated actions:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Physical Safety"}),": Avoiding collisions and dangerous movements"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Social Safety"}),": Respecting personal space and privacy"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Operational Safety"}),": Preventing damage to robot or environment"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Ethical Safety"}),": Following ethical guidelines and user preferences"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"validation-techniques",children:"Validation Techniques"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Rule-Based Checking"}),": Validate against predefined safety rules"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Simulation Verification"}),": Test action sequences in simulation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Human-in-the-Loop"}),": Require human approval for complex plans"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Gradual Execution"}),": Execute plans step-by-step with monitoring"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"example-safety-validation",children:"Example Safety Validation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def validate_action(action, robot_state, world_model):\n    """Validate an action for safety and feasibility"""\n    \n    # Check physical safety\n    if action.type == "NAVIGATE_TO":\n        path = plan_path(robot_state.location, action.params["target"])\n        if path_has_obstacles(path):\n            return False, "Path contains obstacles"\n    \n    # Check operational constraints\n    if action.type == "GRASP_OBJECT":\n        obj = get_object_info(action.params["object_name"])\n        if obj.weight > robot_max_load:\n            return False, "Object too heavy to grasp"\n    \n    # Check social constraints\n    if action.type == "NAVIGATE_TO":\n        target = action.params["target"]\n        if is_in_personal_space(target, humans_in_environment):\n            return False, "Target location in personal space"\n    \n    return True, "Action is safe and feasible"\n'})}),"\n",(0,t.jsx)(e.h2,{id:"cognitive-planning-strategies",children:"Cognitive Planning Strategies"}),"\n",(0,t.jsx)(e.h3,{id:"hierarchical-planning",children:"Hierarchical Planning"}),"\n",(0,t.jsx)(e.p,{children:"Breaking down complex tasks into manageable subtasks:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Decomposition"}),": Breaking high-level commands into subtasks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Subtask Orchestration"}),": Coordinating execution of subtasks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Failure Recovery"}),": Handling failures in subtasks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Resource Management"}),": Allocating resources across subtasks"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"contextual-reasoning",children:"Contextual Reasoning"}),"\n",(0,t.jsx)(e.p,{children:"Using environmental and situational context:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Spatial Reasoning"}),": Understanding spatial relationships"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Temporal Reasoning"}),": Understanding time-based constraints"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Social Reasoning"}),": Understanding social conventions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Physical Reasoning"}),": Understanding object properties and physics"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"learning-from-interaction",children:"Learning from Interaction"}),"\n",(0,t.jsx)(e.p,{children:"Improving planning through experience:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Success/Failure Analysis"}),": Learning from plan outcomes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"User Feedback Integration"}),": Incorporating user corrections"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Adaptive Planning"}),": Adjusting planning strategies based on context"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Knowledge Accumulation"}),": Building a knowledge base of successful plans"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"integration-with-perception-systems",children:"Integration with Perception Systems"}),"\n",(0,t.jsx)(e.h3,{id:"real-time-perception-integration",children:"Real-Time Perception Integration"}),"\n",(0,t.jsx)(e.p,{children:"Combining LLM planning with real-time perception:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Object Recognition"}),": Identifying objects during plan execution"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Localization"}),": Updating robot position during navigation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Scene Understanding"}),": Interpreting environment changes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Human Detection"}),": Recognizing human presence and behavior"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"feedback-loops",children:"Feedback Loops"}),"\n",(0,t.jsx)(e.p,{children:"Creating feedback between LLM and perception:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def execute_plan_with_feedback(plan):\n    """Execute plan with perception-based feedback"""\n    for step in plan:\n        # Execute action\n        action_result = execute_action(step)\n        \n        # Update world model based on perception\n        if action_result.perception_data:\n            update_world_model(action_result.perception_data)\n        \n        # Check if plan needs adjustment\n        if check_plan_obsolescence(plan, current_world_model):\n            # Regenerate plan based on new information\n            new_plan = regenerate_plan_with_context(\n                original_command,\n                current_world_model\n            )\n            return execute_plan_with_feedback(new_plan)\n    \n    return "Plan completed successfully"\n'})}),"\n",(0,t.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(e.h3,{id:"caching-strategies",children:"Caching Strategies"}),"\n",(0,t.jsx)(e.p,{children:"Improving response time through caching:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Plan Caching"}),": Storing common action sequences"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Context Caching"}),": Caching relevant world knowledge"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Response Caching"}),": Caching LLM responses for similar queries"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Computational Caching"}),": Caching intermediate results"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"parallel-processing",children:"Parallel Processing"}),"\n",(0,t.jsx)(e.p,{children:"Managing computational resources:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Background Processing"}),": Pre-computing plans for likely commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Asynchronous Execution"}),": Executing independent actions in parallel"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Load Balancing"}),": Distributing computation across available resources"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Priority Management"}),": Prioritizing safety-critical computations"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,t.jsx)(e.h3,{id:"plan-quality-metrics",children:"Plan Quality Metrics"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Completeness"}),": Percentage of tasks successfully completed"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Efficiency"}),": Time and resources required for task completion"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety"}),": Number of safety violations during execution"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Human Satisfaction"}),": User satisfaction with robot behavior"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"llm-performance-metrics",children:"LLM Performance Metrics"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Accuracy"}),": Correctness of generated action sequences"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Consistency"}),": Consistency in handling similar commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Adaptability"}),": Ability to handle new situations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Latency"}),": Time from command to first action"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"LLM Integration"}),": Set up an LLM client for robotic planning:"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Configure your LLM (OpenAI API, Hugging Face, or local model)\n# Create a simple prompt for translating commands to actions\n# Test with simple commands like "Go forward" or "Turn left"\n'})}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Prompt Engineering"}),": Design effective prompts for robotic tasks:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Create a system prompt that defines robot capabilities"}),"\n",(0,t.jsx)(e.li,{children:"Add few-shot examples for common tasks"}),"\n",(0,t.jsx)(e.li,{children:"Include safety constraints in the prompt"}),"\n",(0,t.jsx)(e.li,{children:"Test the system with various commands"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Safety Validation"}),": Implement safety validation for LLM outputs:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Create rules to validate action sequences"}),"\n",(0,t.jsx)(e.li,{children:"Check for physical safety"}),"\n",(0,t.jsx)(e.li,{children:"Verify operational constraints"}),"\n",(0,t.jsx)(e.li,{children:"Test with potentially unsafe commands"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"ROS Integration"}),": Connect LLM planner to ROS 2 system:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Create a service that accepts natural language commands"}),"\n",(0,t.jsx)(e.li,{children:"Publish generated action sequences to appropriate topics"}),"\n",(0,t.jsx)(e.li,{children:"Integrate with existing navigation and manipulation systems"}),"\n",(0,t.jsx)(e.li,{children:"Test the complete system with simple tasks"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Performance Testing"}),": Evaluate the system performance:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Measure response time for different command types"}),"\n",(0,t.jsx)(e.li,{children:"Test accuracy with similar but different commands"}),"\n",(0,t.jsx)(e.li,{children:"Analyze resource usage during operation"}),"\n",(0,t.jsx)(e.li,{children:"Document any safety issues encountered"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Prompt Design"}),": Design a prompt for a specific robotic task (e.g., setting a table). How would you structure it to ensure safe and effective behavior?"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Safety System"}),": How would you design a safety system that allows LLMs flexibility while preventing dangerous actions?"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Context Integration"}),": How would you integrate real-time perception data into the LLM planning process to handle dynamic environments?"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Error Recovery"}),": Design a strategy for LLM-based planning that handles execution failures gracefully."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"LLM-based cognitive planning enables humanoid robots to interpret and execute natural language commands through sophisticated reasoning capabilities. Proper integration with ROS 2 systems, safety validation, and performance optimization are essential for reliable operation."}),"\n",(0,t.jsx)(e.h2,{id:"self-assessment",children:"Self-Assessment"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"What are the key components of LLM-based robotic planning?"}),"\n",(0,t.jsx)(e.li,{children:"How do you design effective prompts for robotic applications?"}),"\n",(0,t.jsx)(e.li,{children:"What safety considerations are important for LLM-based planning?"}),"\n",(0,t.jsx)(e.li,{children:"How do you integrate LLM planning with ROS 2 action systems?"}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}}}]);