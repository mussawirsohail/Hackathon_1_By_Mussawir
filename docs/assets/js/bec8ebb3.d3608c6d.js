"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[5597],{4150:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vision-language-action/lesson-4-capstone-project","title":"Lesson 4 - Capstone Project - The Autonomous Humanoid","description":"Implementing a complete autonomous humanoid system that responds to voice commands","source":"@site/docs/module-4-vision-language-action/lesson-4-capstone-project.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/lesson-4-capstone-project","permalink":"/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-4-capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-4-capstone-project.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Lesson 4 - Capstone Project - The Autonomous Humanoid","sidebar_position":5,"description":"Implementing a complete autonomous humanoid system that responds to voice commands","learning_objectives":["Integrate all components from previous modules into a complete system","Implement voice command processing, planning, and execution","Handle complex scenarios with multiple requirements","Validate and test the complete autonomous humanoid system"],"duration":200},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3 - Cognitive Planning with LLMs","permalink":"/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-3-cognitive-planning-llms"}}');var o=s(4848),i=s(8453);const a={title:"Lesson 4 - Capstone Project - The Autonomous Humanoid",sidebar_position:5,description:"Implementing a complete autonomous humanoid system that responds to voice commands",learning_objectives:["Integrate all components from previous modules into a complete system","Implement voice command processing, planning, and execution","Handle complex scenarios with multiple requirements","Validate and test the complete autonomous humanoid system"],duration:200},l="Lesson 4 - Capstone Project - The Autonomous Humanoid",r={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Project Overview",id:"project-overview",level:2},{value:"System Architecture",id:"system-architecture",level:3},{value:"Key Components Integration",id:"key-components-integration",level:3},{value:"System Integration",id:"system-integration",level:2},{value:"Main Control Node",id:"main-control-node",level:3},{value:"Integration Architecture",id:"integration-architecture",level:3},{value:"Implementation Scenarios",id:"implementation-scenarios",level:2},{value:"Scenario 1: Basic Object Retrieval",id:"scenario-1-basic-object-retrieval",level:3},{value:"Scenario 2: Complex Multi-Step Task",id:"scenario-2-complex-multi-step-task",level:3},{value:"Scenario 3: Social Navigation",id:"scenario-3-social-navigation",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Unit Testing",id:"unit-testing",level:3},{value:"Integration Testing",id:"integration-testing",level:3},{value:"Validation Metrics",id:"validation-metrics",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Computational Efficiency",id:"computational-efficiency",level:3},{value:"Real-time Considerations",id:"real-time-considerations",level:3},{value:"Debugging and Troubleshooting",id:"debugging-and-troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Diagnostic Tools",id:"diagnostic-tools",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Self-Assessment",id:"self-assessment",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"lesson-4---capstone-project---the-autonomous-humanoid",children:"Lesson 4 - Capstone Project - The Autonomous Humanoid"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this lesson, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate all components from previous modules into a complete system"}),"\n",(0,o.jsx)(n.li,{children:"Implement voice command processing, planning, and execution"}),"\n",(0,o.jsx)(n.li,{children:"Handle complex scenarios with multiple requirements"}),"\n",(0,o.jsx)(n.li,{children:"Validate and test the complete autonomous humanoid system"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"The capstone project brings together all the concepts from the previous modules to create a complete autonomous humanoid system. This system will receive voice commands, process them to understand the user's intent, plan a sequence of actions, and execute those actions in the physical or simulated environment. The project demonstrates the integration of ROS 2 fundamentals, simulation, AI perception, and Vision-Language-Action systems."}),"\n",(0,o.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,o.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,o.jsx)(n.p,{children:"The complete autonomous humanoid system includes:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"[Voice Command] \u2192 [Speech Recognition] \u2192 [NLU] \u2192 [Cognitive Planning] \u2192 [Action Execution] \u2192 [Environment]\n     \u2193              \u2193                    \u2193         \u2193                    \u2193                   \u2193\n[Whisper]     [Intent Analysis]    [LLM Plan] [ROS Actions]      [Gazebo/Reality]    [Result]\n"})}),"\n",(0,o.jsx)(n.h3,{id:"key-components-integration",children:"Key Components Integration"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Processing Layer"}),": OpenAI Whisper for speech recognition"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Natural Language Understanding"}),": LLM-based intent and entity extraction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognitive Planning"}),": High-level action planning and reasoning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation System"}),": Nav2-based path planning and obstacle avoidance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulation System"}),": Object identification and grasping"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulation Environment"}),": Gazebo for testing and validation"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"system-integration",children:"System Integration"}),"\n",(0,o.jsx)(n.h3,{id:"main-control-node",children:"Main Control Node"}),"\n",(0,o.jsx)(n.p,{children:"Creating the central coordinating node:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\nimport rospy\nimport actionlib\nfrom std_msgs.msg import String, Bool\nfrom geometry_msgs.msg import PoseStamped\nfrom move_base_msgs.msg import MoveBaseAction, MoveBaseGoal\nfrom manipulation_msgs.msg import GraspAction, GraspGoal\nfrom audio_common_msgs.msg import AudioData\nfrom capstone_msgs.msg import SystemState\nfrom capstone_msgs.srv import ExecuteCommand, ExecuteCommandResponse\n\nclass AutonomousHumanoidNode:\n    def __init__(self):\n        rospy.init_node(\'autonomous_humanoid\')\n        \n        # Publishers and subscribers\n        self.state_pub = rospy.Publisher(\'system_state\', SystemState, queue_size=1)\n        self.voice_cmd_sub = rospy.Subscriber(\'voice_command\', String, self.voice_command_callback)\n        self.whisper_sub = rospy.Subscriber(\'transcribed_text\', String, self.transcription_callback)\n        \n        # Action clients\n        self.move_base_client = actionlib.SimpleActionClient(\'move_base\', MoveBaseAction)\n        self.grasp_client = actionlib.SimpleActionClient(\'grasp_server\', GraspAction)\n        \n        # Services\n        self.execute_srv = rospy.Service(\'execute_command\', ExecuteCommand, self.execute_command)\n        \n        # Internal state\n        self.current_command = None\n        self.system_state = SystemState()\n        self.system_state.status = "IDLE"\n        \n        # Wait for action servers\n        rospy.loginfo("Waiting for action servers...")\n        self.move_base_client.wait_for_server()\n        self.grasp_client.wait_for_server()\n        \n        rospy.loginfo("Autonomous Humanoid system initialized")\n    \n    def voice_command_callback(self, msg):\n        """Handle voice command from user"""\n        rospy.loginfo(f"Received voice command: {msg.data}")\n        self.system_state.status = "PROCESSING"\n        self.state_pub.publish(self.system_state)\n        \n        # Send command to planning system\n        self.process_command(msg.data)\n    \n    def transcription_callback(self, msg):\n        """Handle transcribed text from Whisper"""\n        rospy.loginfo(f"Transcribed: {msg.data}")\n        # Process the transcribed command\n        self.process_command(msg.data)\n    \n    def process_command(self, command_text):\n        """Process natural language command"""\n        try:\n            # Use LLM to interpret command and generate plan\n            plan = self.generate_plan_from_command(command_text)\n            \n            # Execute the plan\n            success = self.execute_plan(plan)\n            \n            if success:\n                self.system_state.status = "IDLE"\n                rospy.loginfo("Command executed successfully")\n            else:\n                self.system_state.status = "ERROR"\n                rospy.logerr("Command execution failed")\n                \n        except Exception as e:\n            rospy.logerr(f"Error processing command: {e}")\n            self.system_state.status = "ERROR"\n    \n    def generate_plan_from_command(self, command):\n        """Generate action plan from natural language command"""\n        # This would interface with your LLM-based planning system\n        # For this example, we\'ll handle some basic commands\n        command_lower = command.lower()\n        \n        if "go to" in command_lower:\n            location = self.extract_location(command)\n            return [("navigate", location)]\n        elif "pick up" in command_lower or "grasp" in command_lower:\n            object_name = self.extract_object(command)\n            return [("grasp", object_name)]\n        elif "bring me" in command_lower:\n            parts = command_lower.split("bring me")\n            object_name = self.extract_object(parts[1])\n            return [("grasp", object_name), ("navigate", "user")]\n        else:\n            # Use LLM for complex command planning\n            return self.request_complex_plan(command)\n    \n    def execute_plan(self, plan):\n        """Execute the action plan"""\n        try:\n            for action_type, params in plan:\n                if action_type == "navigate":\n                    success = self.navigate_to_location(params)\n                elif action_type == "grasp":\n                    success = self.grasp_object(params)\n                else:\n                    rospy.logwarn(f"Unknown action type: {action_type}")\n                    success = False\n                \n                if not success:\n                    rospy.logerr(f"Action failed: {action_type} {params}")\n                    return False\n            \n            return True\n        except Exception as e:\n            rospy.logerr(f"Error executing plan: {e}")\n            return False\n    \n    def navigate_to_location(self, location):\n        """Navigate to a specific location"""\n        # Convert location name to coordinates (would use map lookup)\n        pose = self.location_to_pose(location)\n        \n        if not pose:\n            rospy.logerr(f"Unknown location: {location}")\n            return False\n        \n        goal = MoveBaseGoal()\n        goal.target_pose = pose\n        \n        rospy.loginfo(f"Navigating to {location}")\n        self.move_base_client.send_goal(goal)\n        \n        # Wait for result with timeout\n        finished_within_time = self.move_base_client.wait_for_result(rospy.Duration(60.0))\n        \n        if not finished_within_time:\n            rospy.logerr("Navigation took too long")\n            self.move_base_client.cancel_goal()\n            return False\n        \n        state = self.move_base_client.get_state()\n        success = (state == actionlib.GoalStatus.SUCCEEDED)\n        \n        if success:\n            rospy.loginfo(f"Successfully reached {location}")\n        else:\n            rospy.logerr(f"Failed to reach {location}")\n        \n        return success\n    \n    def grasp_object(self, object_name):\n        """Grasp a specific object"""\n        goal = GraspGoal()\n        goal.object_name = object_name\n        \n        rospy.loginfo(f"Attempting to grasp {object_name}")\n        self.grasp_client.send_goal(goal)\n        \n        # Wait for result with timeout\n        finished_within_time = self.grasp_client.wait_for_result(rospy.Duration(30.0))\n        \n        if not finished_within_time:\n            rospy.logerr(f"Grasping {object_name} took too long")\n            self.grasp_client.cancel_goal()\n            return False\n        \n        state = self.grasp_client.get_state()\n        success = (state == actionlib.GoalStatus.SUCCEEDED)\n        \n        if success:\n            rospy.loginfo(f"Successfully grasped {object_name}")\n        else:\n            rospy.logerr(f"Failed to grasp {object_name}")\n        \n        return success\n    \n    def execute_command(self, req):\n        """Service to execute command directly"""\n        success = self.process_command_with_context(req.command, req.context)\n        return ExecuteCommandResponse(success=success)\n    \n    def location_to_pose(self, location_name):\n        """Convert location name to PoseStamped"""\n        # This would typically look up coordinates from a map\n        # For this example, we\'ll use hardcoded locations\n        locations = {\n            "kitchen": PoseStamped(),  # Filled with appropriate coordinates\n            "living room": PoseStamped(),\n            "bedroom": PoseStamped(),\n            "user": self.get_user_pose()  # Would get current user position\n        }\n        \n        return locations.get(location_name.lower())\n    \n    def get_user_pose(self):\n        """Get current user position (placeholder)"""\n        # In a real system, this might use person detection or tracking\n        pose = PoseStamped()\n        # Set appropriate coordinates\n        return pose\n    \n    def extract_location(self, command):\n        """Extract location from command (simplified)"""\n        # This would use more sophisticated NLU in practice\n        if "kitchen" in command.lower():\n            return "kitchen"\n        elif "living room" in command.lower():\n            return "living room"\n        elif "bedroom" in command.lower():\n            return "bedroom"\n        else:\n            return "unknown"\n    \n    def extract_object(self, command):\n        """Extract object from command (simplified)"""\n        # This would use more sophisticated NLU in practice\n        # For now, just return a simple object\n        if "cup" in command.lower():\n            return "cup"\n        elif "book" in command.lower():\n            return "book"\n        else:\n            return "object"\n    \n    def request_complex_plan(self, command):\n        """Request complex plan from LLM system"""\n        # Interface with your LLM planning system\n        # This would send the command to your LLM planning system\n        # and receive back a sequence of actions\n        rospy.loginfo(f"Requesting plan for: {command}")\n        # Placeholder - would call LLM planning system\n        return [("navigate", "location"), ("grasp", "object")]\n    \n    def run(self):\n        """Run the main loop"""\n        rate = rospy.Rate(1)  # 1 Hz\n        while not rospy.is_shutdown():\n            # Update system state\n            self.state_pub.publish(self.system_state)\n            rate.sleep()\n\nif __name__ == \'__main__\':\n    node = AutonomousHumanoidNode()\n    node.run()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"integration-architecture",children:"Integration Architecture"}),"\n",(0,o.jsx)(n.p,{children:"Creating launch files to bring up the complete system:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'\x3c!-- capstone_launch.xml --\x3e\n<launch>\n  \x3c!-- Simulation environment --\x3e\n  <include file="$(find gazebo_ros)/launch/empty_world.launch">\n    <arg name="world_name" value="$(find capstone_project)/worlds/indoor_world.world" />\n  </include>\n  \n  \x3c!-- Robot spawn --\x3e\n  <node name="spawn_urdf" pkg="gazebo_ros" type="spawn_model" args="-file $(find capstone_project)/robots/humanoid.urdf -urdf -model humanoid_robot" />\n  \n  \x3c!-- Navigation stack --\x3e\n  <include file="$(find nav2_bringup)/launch/navigation_launch.py">\n    <arg name="use_sim_time" value="True"/>\n  </include>\n  \n  \x3c!-- Speech recognition --\x3e\n  <node pkg="audio_capture" exec="audio_capture_node" name="audio_capture" output="screen">\n    <param name="device" value="default" />\n    <param name="sample_rate" value="16000" />\n  </node>\n  \n  <node pkg="whisper_ros" exec="whisper_node" name="whisper_node" output="screen">\n    <param name="model_size" value="base" />\n    <param name="language" value="en" />\n  </node>\n  \n  \x3c!-- Cognitive planning --\x3e\n  <node pkg="llm_planner" exec="llm_planner_node" name="llm_planner" output="screen">\n    <param name="model_name" value="gpt-3.5-turbo" />\n  </node>\n  \n  \x3c!-- Main control node --\x3e\n  <node pkg="capstone_project" exec="autonomous_humanoid_node" name="autonomous_humanoid" output="screen">\n  </node>\n  \n  \x3c!-- Visualization --\x3e\n  <node name="rviz" pkg="rviz2" exec="rviz2" args="-d $(find capstone_project)/rviz/capstone_config.rviz" />\n</launch>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"implementation-scenarios",children:"Implementation Scenarios"}),"\n",(0,o.jsx)(n.h3,{id:"scenario-1-basic-object-retrieval",children:"Scenario 1: Basic Object Retrieval"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Command"}),': "Go to the kitchen and bring me a cup"']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Execution Flow"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Voice command captured and transcribed"}),"\n",(0,o.jsx)(n.li,{children:'Intent identified as "fetch_object" with location "kitchen" and object "cup"'}),"\n",(0,o.jsx)(n.li,{children:"Navigation system plans path to kitchen"}),"\n",(0,o.jsx)(n.li,{children:"Robot navigates to kitchen and identifies available cups"}),"\n",(0,o.jsx)(n.li,{children:"Manipulation system plans and executes grasp of cup"}),"\n",(0,o.jsx)(n.li,{children:"Robot navigates back to user"}),"\n",(0,o.jsx)(n.li,{children:"Manipulation system places cup near user"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"scenario-2-complex-multi-step-task",children:"Scenario 2: Complex Multi-Step Task"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Command"}),': "Clean the table in the living room"']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Execution Flow"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"LLM-based planning decomposes task into sub-tasks"}),"\n",(0,o.jsx)(n.li,{children:"Navigate to living room table"}),"\n",(0,o.jsx)(n.li,{children:"Identify objects on table"}),"\n",(0,o.jsxs)(n.li,{children:["For each movable object:","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Identify appropriate storage location"}),"\n",(0,o.jsx)(n.li,{children:"Grasp object"}),"\n",(0,o.jsx)(n.li,{children:"Navigate to storage location"}),"\n",(0,o.jsx)(n.li,{children:"Place object"}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.li,{children:"Verify table is clear"}),"\n",(0,o.jsx)(n.li,{children:"Report completion"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"scenario-3-social-navigation",children:"Scenario 3: Social Navigation"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Command"}),': "Go to the front door, but don\'t bump into anyone"']}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Execution Flow"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Identify front door location"}),"\n",(0,o.jsx)(n.li,{children:"Plan path that avoids obstacles and people"}),"\n",(0,o.jsx)(n.li,{children:"Use social navigation behaviors to respect personal space"}),"\n",(0,o.jsx)(n.li,{children:"Navigate while monitoring for dynamic obstacles"}),"\n",(0,o.jsx)(n.li,{children:"Adjust path planning in real-time as people move"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,o.jsx)(n.h3,{id:"unit-testing",children:"Unit Testing"}),"\n",(0,o.jsx)(n.p,{children:"Test individual components:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import unittest\nimport rospy\nfrom capstone_msgs.srv import ExecuteCommand\n\nclass TestAutonomousHumanoid(unittest.TestCase):\n    def setUp(self):\n        rospy.init_node(\'test_autonomous_humanoid\', anonymous=True)\n        rospy.wait_for_service(\'execute_command\')\n        self.execute_command = rospy.ServiceProxy(\'execute_command\', ExecuteCommand)\n    \n    def test_simple_navigation(self):\n        """Test simple navigation command"""\n        response = self.execute_command("Go to the kitchen", "{}")\n        self.assertTrue(response.success)\n    \n    def test_object_fetch(self):\n        """Test object fetching command"""\n        response = self.execute_command("Bring me a cup", "{}")\n        self.assertTrue(response.success)\n    \n    def test_complex_task(self):\n        """Test complex multi-step task"""\n        response = self.execute_command("Clean the table", "{}")\n        self.assertTrue(response.success)\n\nif __name__ == \'__main__\':\n    import rosunit\n    rosunit.unitrun(\'capstone_project\', \'test_autonomous_humanoid\', TestAutonomousHumanoid)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,o.jsx)(n.p,{children:"Test complete system functionality:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'# Test scenarios for integration testing\ntest_scenarios:\n  - name: "basic_navigation"\n    command: "Go to the kitchen"\n    expected: ["navigation_success"]\n    timeout: 60\n    \n  - name: "object_retrieval"\n    command: "Bring me a cup"\n    expected: ["navigation_to_kitchen", "object_grasped", "navigation_to_user", "object_delivered"]\n    timeout: 120\n    \n  - name: "multi_step_task"\n    command: "Go to living room, pick up the book, and bring it to me"\n    expected: ["navigation_to_living_room", "object_grasped", "navigation_to_user", "object_delivered"]\n    timeout: 180\n    \n  - name: "social_navigation"\n    command: "Navigate to front door while avoiding people"\n    expected: ["navigation_with_avoidance", "safe_path_completion"]\n    timeout: 90\n'})}),"\n",(0,o.jsx)(n.h3,{id:"validation-metrics",children:"Validation Metrics"}),"\n",(0,o.jsx)(n.p,{children:"Track system performance:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Task Success Rate"}),": Percentage of tasks completed successfully"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Time Efficiency"}),": Time taken to complete tasks vs. optimal"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety Metrics"}),": Number of safety violations or near-misses"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"User Satisfaction"}),": User feedback on system performance"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"System Reliability"}),": Time between system failures"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"computational-efficiency",children:"Computational Efficiency"}),"\n",(0,o.jsx)(n.p,{children:"Optimize system performance:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Parallel Processing"}),": Execute independent tasks simultaneously"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Caching"}),": Cache frequently used plans and information"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model Optimization"}),": Use quantized or distilled models where possible"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource Management"}),": Monitor and balance computational resources"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"real-time-considerations",children:"Real-time Considerations"}),"\n",(0,o.jsx)(n.p,{children:"Ensure real-time operation:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Latency Management"}),": Minimize delays between command and action"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Priority Scheduling"}),": Ensure safety-critical operations take precedence"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Error Recovery"}),": Gracefully handle failures without system crashes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"State Monitoring"}),": Continuously monitor system state for anomalies"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"debugging-and-troubleshooting",children:"Debugging and Troubleshooting"}),"\n",(0,o.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition Errors"}),": Implement confidence thresholds and re-asking mechanisms"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation Failures"}),": Use multiple planning strategies and recovery behaviors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Detection Issues"}),": Implement robust object identification with multiple sensors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Communication Problems"}),": Ensure ROS communication reliability"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"diagnostic-tools",children:"Diagnostic Tools"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Diagnostic node to monitor system health\nimport rospy\nfrom diagnostic_msgs.msg import DiagnosticArray, DiagnosticStatus, KeyValue\n\nclass CapstoneDiagnostics:\n    def __init__(self):\n        rospy.init_node(\'capstone_diagnostics\')\n        self.diag_pub = rospy.Publisher(\'/diagnostics\', DiagnosticArray, queue_size=1)\n        self.timer = rospy.Timer(rospy.Duration(1.0), self.publish_diagnostics)\n        \n    def publish_diagnostics(self, event):\n        msg = DiagnosticArray()\n        msg.header.stamp = rospy.Time.now()\n        \n        # System health\n        status = DiagnosticStatus()\n        status.name = "Capstone System Health"\n        status.level = DiagnosticStatus.OK\n        status.message = "All systems operational"\n        \n        # Add key-value pairs for system stats\n        status.values.append(KeyValue("navigation_status", self.get_navigation_status()))\n        status.values.append(KeyValue("manipulation_status", self.get_manipulation_status()))\n        status.values.append(KeyValue("speech_recognition_status", self.get_speech_status()))\n        status.values.append(KeyValue("battery_level", self.get_battery_level()))\n        \n        msg.status.append(status)\n        self.diag_pub.publish(msg)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"System Assembly"}),": Assemble the complete system using previously created components:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Launch all required nodes (simulation, speech, planning, control)"}),"\n",(0,o.jsx)(n.li,{children:"Verify that all components can communicate"}),"\n",(0,o.jsx)(n.li,{children:"Test individual subsystems before integration"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Basic Command Test"}),": Test the system with simple commands:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'"Go to the kitchen"'}),"\n",(0,o.jsx)(n.li,{children:'"Move forward"'}),"\n",(0,o.jsx)(n.li,{children:'"Turn left"'}),"\n",(0,o.jsx)(n.li,{children:"Verify each command executes correctly"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Object Retrieval"}),": Test the complete object retrieval scenario:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'"Go to the kitchen and bring me a cup"'}),"\n",(0,o.jsx)(n.li,{children:"Monitor the entire process: navigation, object detection, grasping, and delivery"}),"\n",(0,o.jsx)(n.li,{children:"Verify safety and success criteria"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Complex Task Execution"}),": Execute the complex multi-step task:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'"Clean the table"'}),"\n",(0,o.jsx)(n.li,{children:"Monitor decomposition, planning, and execution of multiple subtasks"}),"\n",(0,o.jsx)(n.li,{children:"Verify the completion criteria are met"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Performance Evaluation"}),": Evaluate system performance:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Measure task success rates"}),"\n",(0,o.jsx)(n.li,{children:"Track time efficiency"}),"\n",(0,o.jsx)(n.li,{children:"Assess user satisfaction"}),"\n",(0,o.jsx)(n.li,{children:"Document any system limitations or issues"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Error Handling"}),": How would you implement robust error handling for when the robot fails to grasp an object or navigate to a location?"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"System Scaling"}),": How would you modify the system to handle multiple simultaneous commands or operate with multiple robots?"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Learning Integration"}),": How could you add learning capabilities to improve system performance based on experience?"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Real-World Deployment"}),": What additional considerations would be needed to deploy this system in a real-world environment?"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"The capstone project demonstrates the integration of all components from the Physical AI & Humanoid Robotics course. It showcases how ROS 2 fundamentals, simulation, AI perception systems, and Vision-Language-Action frameworks combine to create an intelligent, autonomous humanoid robot capable of understanding and executing natural language commands."}),"\n",(0,o.jsx)(n.h2,{id:"self-assessment",children:"Self-Assessment"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"How do the different modules of this course integrate in the capstone project?"}),"\n",(0,o.jsx)(n.li,{children:"What are the main challenges in creating an autonomous humanoid system?"}),"\n",(0,o.jsx)(n.li,{children:"How would you validate the safety and reliability of such a system?"}),"\n",(0,o.jsx)(n.li,{children:"What are the key performance metrics for evaluating the complete system?"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>l});var t=s(6540);const o={},i=t.createContext(o);function a(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);