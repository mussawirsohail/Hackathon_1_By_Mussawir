"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[804],{1791:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"module-4-vision-language-action/lesson-2-voice-to-action-whisper","title":"Lesson 2 - Voice-to-Action with OpenAI Whisper","description":"Using OpenAI Whisper for voice command processing in humanoid robots","source":"@site/docs/module-4-vision-language-action/lesson-2-voice-to-action-whisper.md","sourceDirName":"module-4-vision-language-action","slug":"/module-4-vision-language-action/lesson-2-voice-to-action-whisper","permalink":"/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-2-voice-to-action-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-2-voice-to-action-whisper.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Lesson 2 - Voice-to-Action with OpenAI Whisper","sidebar_position":3,"description":"Using OpenAI Whisper for voice command processing in humanoid robots","learning_objectives":["Configure OpenAI Whisper for robotic voice command processing","Implement speech-to-text pipelines for robot control","Integrate voice processing with ROS 2 systems","Optimize voice processing for real-time robotic applications"],"duration":150},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1 - Introduction to Vision-Language-Action Systems","permalink":"/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-1-introduction-to-vla"},"next":{"title":"Lesson 3 - Cognitive Planning with LLMs","permalink":"/Hackathon_1_By_Mussawir/docs/module-4-vision-language-action/lesson-3-cognitive-planning-llms"}}');var o=i(4848),r=i(8453);const t={title:"Lesson 2 - Voice-to-Action with OpenAI Whisper",sidebar_position:3,description:"Using OpenAI Whisper for voice command processing in humanoid robots",learning_objectives:["Configure OpenAI Whisper for robotic voice command processing","Implement speech-to-text pipelines for robot control","Integrate voice processing with ROS 2 systems","Optimize voice processing for real-time robotic applications"],duration:150},l="Lesson 2 - Voice-to-Action with OpenAI Whisper",c={},a=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"OpenAI Whisper Overview",id:"openai-whisper-overview",level:2},{value:"Architecture",id:"architecture",level:3},{value:"Model Variants",id:"model-variants",level:3},{value:"Advantages for Robotics",id:"advantages-for-robotics",level:3},{value:"Voice Command Pipeline for Robotics",id:"voice-command-pipeline-for-robotics",level:2},{value:"Audio Capture",id:"audio-capture",level:3},{value:"Preprocessing",id:"preprocessing",level:3},{value:"Processing Flow",id:"processing-flow",level:3},{value:"Implementation in Robotic Systems",id:"implementation-in-robotic-systems",level:2},{value:"Basic Whisper Integration",id:"basic-whisper-integration",level:3},{value:"Real-Time Processing Considerations",id:"real-time-processing-considerations",level:3},{value:"Robot Integration Challenges",id:"robot-integration-challenges",level:2},{value:"Acoustic Challenges",id:"acoustic-challenges",level:3},{value:"Processing Challenges",id:"processing-challenges",level:3},{value:"Whisper Optimization for Robotics",id:"whisper-optimization-for-robotics",level:2},{value:"Model Quantization",id:"model-quantization",level:3},{value:"Hardware Acceleration",id:"hardware-acceleration",level:3},{value:"Selective Processing",id:"selective-processing",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Message Types",id:"message-types",level:3},{value:"Example Launch Configuration",id:"example-launch-configuration",level:3},{value:"Voice Command Processing",id:"voice-command-processing",level:2},{value:"Command Recognition",id:"command-recognition",level:3},{value:"Example Command Processing",id:"example-command-processing",level:3},{value:"Quality Assurance",id:"quality-assurance",level:2},{value:"Accuracy Monitoring",id:"accuracy-monitoring",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Self-Assessment",id:"self-assessment",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"lesson-2---voice-to-action-with-openai-whisper",children:"Lesson 2 - Voice-to-Action with OpenAI Whisper"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this lesson, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Configure OpenAI Whisper for robotic voice command processing"}),"\n",(0,o.jsx)(n.li,{children:"Implement speech-to-text pipelines for robot control"}),"\n",(0,o.jsx)(n.li,{children:"Integrate voice processing with ROS 2 systems"}),"\n",(0,o.jsx)(n.li,{children:"Optimize voice processing for real-time robotic applications"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI Whisper is a state-of-the-art automatic speech recognition (ASR) system that can convert spoken language to text with high accuracy. For humanoid robots, integrating Whisper enables natural voice-based interaction, allowing users to issue commands in natural language. This lesson covers implementing Whisper for robotic applications, handling the unique challenges of real-time processing, and integrating with the broader robotic system."}),"\n",(0,o.jsx)(n.h2,{id:"openai-whisper-overview",children:"OpenAI Whisper Overview"}),"\n",(0,o.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,o.jsx)(n.p,{children:"Whisper is a transformer-based model that includes both an encoder-decoder structure:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Encoder"}),": Processes audio input and extracts features"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decoder"}),": Generates text based on encoded features"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multilingual Capability"}),": Can process multiple languages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Handles various accents, background noise, and audio qualities"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"model-variants",children:"Model Variants"}),"\n",(0,o.jsx)(n.p,{children:"Whisper is available in different sizes with trade-offs between accuracy and computational requirements:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"tiny"}),": Fastest, smallest model with lower accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"base"}),": Good balance of speed and accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"small"}),": Better accuracy with moderate computational cost"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"medium"}),": High accuracy with significant computational requirements"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"large"}),": Highest accuracy, most computationally expensive"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"advantages-for-robotics",children:"Advantages for Robotics"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness"}),": Handles various acoustic conditions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multilingual"}),": Supports multiple languages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Punctuation"}),": Automatically adds punctuation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Timestamps"}),": Provides word-level timing information"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"voice-command-pipeline-for-robotics",children:"Voice Command Pipeline for Robotics"}),"\n",(0,o.jsx)(n.h3,{id:"audio-capture",children:"Audio Capture"}),"\n",(0,o.jsx)(n.p,{children:"The first step in voice processing is capturing audio:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Microphone Array"}),": Multiple microphones for improved sound capture"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Noise Reduction"}),": Filtering ambient noise"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Beamforming"}),": Focusing on speaker's voice direction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Echo Cancellation"}),": Removing robot's own speech from input"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"preprocessing",children:"Preprocessing"}),"\n",(0,o.jsx)(n.p,{children:"Audio preprocessing for Whisper:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resampling"}),": Converting audio to required sampling rate (16kHz)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Normalization"}),": Adjusting audio levels"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Filtering"}),": Removing unwanted frequencies"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"VAD (Voice Activity Detection)"}),": Detecting when speech begins/ends"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"processing-flow",children:"Processing Flow"}),"\n",(0,o.jsx)(n.p,{children:"The complete processing pipeline:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Audio Input \u2192 Preprocessing \u2192 Whisper \u2192 Text Output \u2192 NLU \u2192 Action Planning\n"})}),"\n",(0,o.jsx)(n.h2,{id:"implementation-in-robotic-systems",children:"Implementation in Robotic Systems"}),"\n",(0,o.jsx)(n.h3,{id:"basic-whisper-integration",children:"Basic Whisper Integration"}),"\n",(0,o.jsx)(n.p,{children:"Using Whisper in a robotic application:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import whisper\nimport rospy\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\n\nclass WhisperNode:\n    def __init__(self):\n        rospy.init_node(\'whisper_node\')\n        \n        # Load Whisper model\n        self.model = whisper.load_model("base")\n        \n        # Audio input subscriber\n        self.audio_sub = rospy.Subscriber("/audio", AudioData, self.audio_callback)\n        \n        # Text output publisher\n        self.text_pub = rospy.Publisher("/transcribed_text", String, queue_size=10)\n        \n        rospy.loginfo("Whisper node initialized")\n    \n    def audio_callback(self, audio_msg):\n        # Convert audio data to numpy array\n        audio_array = np.frombuffer(audio_msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n        \n        # Transcribe using Whisper\n        result = self.model.transcribe(audio_array)\n        \n        # Publish transcribed text\n        text_msg = String()\n        text_msg.data = result["text"]\n        self.text_pub.publish(text_msg)\n        \n        rospy.loginfo(f"Transcribed: {result[\'text\']}")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"real-time-processing-considerations",children:"Real-Time Processing Considerations"}),"\n",(0,o.jsx)(n.p,{children:"For real-time robotic applications:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Buffer Management"}),": Using rolling buffers to process continuous audio"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Latency Optimization"}),": Minimizing delay between speech and action"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource Management"}),": Balancing CPU/GPU usage with other robot tasks"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Fallback Mechanisms"}),": Handling processing failures gracefully"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"robot-integration-challenges",children:"Robot Integration Challenges"}),"\n",(0,o.jsx)(n.h3,{id:"acoustic-challenges",children:"Acoustic Challenges"}),"\n",(0,o.jsx)(n.p,{children:"Robots present unique acoustic challenges:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Self-Noise"}),": Fan noise, servo sounds, and other robot-generated noise"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Movement Noise"}),": Sounds from robot locomotion affecting audio input"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environmental Noise"}),": Different acoustic conditions in various environments"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"processing-challenges",children:"Processing Challenges"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-Time Constraints"}),": Processing speech in near real-time"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Continuous Operation"}),": Running 24/7 without degradation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource Competition"}),": Sharing computational resources with other robot systems"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Power Management"}),": Managing power consumption of processing systems"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"whisper-optimization-for-robotics",children:"Whisper Optimization for Robotics"}),"\n",(0,o.jsx)(n.h3,{id:"model-quantization",children:"Model Quantization"}),"\n",(0,o.jsx)(n.p,{children:"Reducing model size and computational requirements:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"INT8 Quantization"}),": Converting weights to 8-bit integers"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pruning"}),": Removing less important weights"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Knowledge Distillation"}),": Creating smaller, faster student models"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"hardware-acceleration",children:"Hardware Acceleration"}),"\n",(0,o.jsx)(n.p,{children:"Leveraging specialized hardware:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"GPU Inference"}),": Using GPU for faster transcription"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"TensorRT"}),": NVIDIA's optimization framework"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Edge AI Chips"}),": Specialized processors for AI inference"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"selective-processing",children:"Selective Processing"}),"\n",(0,o.jsx)(n.p,{children:"Optimizing by processing only necessary data:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Voice Activity Detection"}),": Only processing when speech is detected"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Wake Word Recognition"}),": Activating full processing only after specific words"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Context Awareness"}),": Focusing processing on relevant commands"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,o.jsx)(n.h3,{id:"message-types",children:"Message Types"}),"\n",(0,o.jsx)(n.p,{children:"Using appropriate ROS 2 message types:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:"# Audio input\nsensor_msgs/Audio: Raw audio data\n\n# Transcription output\nstd_msgs/String: Transcribed text\n\n# Command output\nstd_msgs/String: Parsed robot commands\n\n# Status\nstd_msgs/Bool: Processing status\n"})}),"\n",(0,o.jsx)(n.h3,{id:"example-launch-configuration",children:"Example Launch Configuration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'\x3c!-- whisper_launch.xml --\x3e\n<launch>\n  \x3c!-- Audio capture node --\x3e\n  <node pkg="audio_capture" exec="audio_capture_node" name="audio_capture">\n    <param name="device" value="default" />\n    <param name="sample_rate" value="16000" />\n  </node>\n  \n  \x3c!-- Whisper transcription node --\x3e\n  <node pkg="whisper_ros" exec="whisper_node" name="whisper_node" output="screen">\n    <param name="model_size" value="base" />\n    <param name="language" value="en" />\n  </node>\n  \n  \x3c!-- Natural language understanding node --\x3e\n  <node pkg="nlu_parser" exec="nlu_node" name="nlu_node">\n    <param name="command_keywords" value="[\'go to\', \'pick up\', \'bring\', \'clean\']" />\n  </node>\n</launch>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"voice-command-processing",children:"Voice Command Processing"}),"\n",(0,o.jsx)(n.h3,{id:"command-recognition",children:"Command Recognition"}),"\n",(0,o.jsx)(n.p,{children:"Processing voice commands for robotic systems:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Keyword Spotting"}),": Identifying robot-specific commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intent Classification"}),": Categorizing user intent"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Entity Extraction"}),": Identifying objects, locations, and parameters"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Confidence Scoring"}),": Assessing reliability of transcription"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"example-command-processing",children:"Example Command Processing"}),"\n",(0,o.jsx)(n.p,{children:'Processing a command like "Go to the kitchen and bring me a cup":'}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition"}),": Convert speech to text using Whisper"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intent Classification"}),': Identify "navigation + manipulation" intent']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Entity Extraction"}),': Extract "kitchen" and "cup"']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Planning"}),": Generate navigation and manipulation sequence"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Execution"}),": Execute the planned actions"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"quality-assurance",children:"Quality Assurance"}),"\n",(0,o.jsx)(n.h3,{id:"accuracy-monitoring",children:"Accuracy Monitoring"}),"\n",(0,o.jsx)(n.p,{children:"Monitoring transcription quality:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Confidence Scores"}),": Tracking model confidence in predictions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Error Detection"}),": Identifying when transcription might be incorrect"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Context Verification"}),": Using world knowledge to verify transcriptions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"User Feedback"}),": Allowing users to correct misrecognitions"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Latency Measurement"}),": Tracking time from speech to action"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Throughput Monitoring"}),": Ensuring system can handle continuous input"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Resource Utilization"}),": Monitoring CPU and memory usage"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Power Consumption"}),": Tracking energy usage for battery-powered robots"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Whisper Installation"}),": Install and configure OpenAI Whisper:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n# Install additional dependencies as needed\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Basic Transcription Node"}),": Create a simple node that transcribes audio:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Create whisper_node.py that subscribes to audio and publishes text\n# Verify it can transcribe simple phrases\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Performance Testing"}),": Test different Whisper models:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Compare accuracy and speed of different model sizes"}),"\n",(0,o.jsx)(n.li,{children:"Measure resource usage and latency"}),"\n",(0,o.jsx)(n.li,{children:"Analyze trade-offs for robotic applications"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Integration Test"}),": Integrate with a simple ROS system:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Create a publisher that sends commands based on transcribed text"}),"\n",(0,o.jsx)(n.li,{children:'Test with simple commands like "move forward", "stop", etc.'}),"\n",(0,o.jsx)(n.li,{children:"Verify the system responds appropriately"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Optimization"}),": Implement optimizations:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use Voice Activity Detection to reduce unnecessary processing"}),"\n",(0,o.jsx)(n.li,{children:"Compare different models and find the best balance of accuracy and speed"}),"\n",(0,o.jsx)(n.li,{children:"Test the system in a noisy environment"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Model Selection"}),": For a battery-powered humanoid robot, which Whisper model would you choose and why? Consider accuracy, computational requirements, and power consumption."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Acoustic Design"}),": How would you design the microphone system to minimize self-noise from the robot's operation?"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Processing Pipeline"}),": Design a complete voice-to-action pipeline that includes error handling and confidence scoring."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Context Awareness"}),": How would you implement a system that uses environmental context to improve command understanding?"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI Whisper enables robust voice command processing for humanoid robots, allowing natural interaction through speech. Understanding its implementation, optimization, and integration with ROS 2 is crucial for developing voice-enabled robotic systems."}),"\n",(0,o.jsx)(n.h2,{id:"self-assessment",children:"Self-Assessment"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"What are the different Whisper model variants and their trade-offs?"}),"\n",(0,o.jsx)(n.li,{children:"What are the key challenges in using Whisper for robotic applications?"}),"\n",(0,o.jsx)(n.li,{children:"How do you optimize Whisper for real-time robotic processing?"}),"\n",(0,o.jsx)(n.li,{children:"How do you integrate Whisper with ROS 2 systems?"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>l});var s=i(6540);const o={},r=s.createContext(o);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);